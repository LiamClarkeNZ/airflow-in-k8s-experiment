apiVersion: v1
kind: ConfigMap
metadata:
  name: airflow-config
  namespace: airflow
data:
  airflow.cfg: |
    [core]
    executor = KubernetesExecutor
    load_examples = False
    load_default_connections = False

    remote_logging = True
    logging_config_class = airflow.config.logging_config.LOGGING_CONFIG

    ## Experiment settings
    # Only to see what's going on
    logging_level = DEBUG
    # Use a real bucket name/path
    remote_base_log_folder = s3://airflow/logs
    # Pull this in via an env var, injected by k8s from secret
    sql_alchemy_conn = postgresql://airflow:password@airflow-postgresql:5432/airflow
    # Pull this in via an env var, injected by k8s from secret
    fernet_key = PUlSbKcKvEHVJew0x6tLlHOIjjTTlpzU8KRJE5vItXM=
    # Only needed to work with localstack
    remote_log_conn_id = localstack_s3
    # DAGs ready to go right away
    dags_are_paused_at_creation = False

    [kubernetes]
    airflow_configmap = airflow-config
    namespace = airflow
    git_repo = https://github.com/jtnz/airflow-in-k8s-experiment.git
    git_branch = master
    git_subpath = dags
    git_dags_folder_mount_point = /opt/airflow/dags

    ## Experiment settings
    # Use real repo
    worker_container_repository = registry.local:5000/base_image
    # Use real tag
    worker_container_tag = latest
    # Only because we're using 'latest' tag
    worker_container_image_pull_policy = Always
    # Only to debug pods
    delete_worker_pods = False

    [scheduler]
    dag_dir_list_interval = 5

    [webserver]
    expose_config = True
